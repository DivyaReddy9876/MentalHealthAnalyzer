# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SEQ58NEgK14yuyRISPlgAi4KXaPFiuOS
"""

# 1. Data Loading
def load_data():
    import pandas as pd
    df = pd.read_excel('Combined Data.xlsx')
    df.columns = ['count', 'statement', 'status']
    print("Original Data Shape:", df.shape)
    return df

df = load_data()

# 2. Before Preprocessing Display
#def show_before_preprocessing(df):
 #   print("\nBefore Preprocessing:")
  #  print("Sample Data:")
   # print(df.head())
    #print("\nMissing Values:", df.isnull().sum())
    #print("Data Types:", df.dtypes)
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder

def preprocess_data(df):
    # Handling missing values
    df = df.dropna()  # Drop rows with missing values

    # Encode the target variable 'status'
    label_encoder = LabelEncoder()
    df['status'] = label_encoder.fit_transform(df['status'])

    # Convert text data into numerical form using TF-IDF Vectorizer
    vectorizer = TfidfVectorizer(stop_words='english')
    X = vectorizer.fit_transform(df['statement'])  # Converts text into a sparse matrix
    y = df['status']

    return X, y  # Returning features and target

show_before_preprocessing(df)

def preprocess_data1(df):
    from sklearn.preprocessing import LabelEncoder
    from sklearn.feature_extraction.text import TfidfVectorizer

    # Handle missing values and clean text
    df['statement'] = df['statement'].fillna('')
    df['status'] = df['status'].fillna('Unknown')
    df['statement'] = df['statement'].astype(str).str.lower()
    df['statement'] = df['statement'].str.replace('[^\w\s]', '')

    # Define the correct order of classes
    class_order = ['Anxiety', 'Bipolar', 'Depression', 'Normal',
                  'Personality disorder', 'Stress', 'Suicidal']

    # Custom label encoding to maintain desired order
    le = LabelEncoder()
    le.fit(class_order)
    df['status_encoded'] = le.transform(df['status'])

    # Verify classes
    print("Target classes after encoding:", le.classes_)

    # Vectorize text
    vectorizer = TfidfVectorizer(max_features=100)
    X = vectorizer.fit_transform(df['statement']).toarray()
    y = df['status_encoded']

    return X, y, le

# Run the updated preprocessing
X, y, le = preprocess_data1(df)

def preprocess_data(df):
    from sklearn.preprocessing import LabelEncoder
    from sklearn.feature_extraction.text import TfidfVectorizer

    # Show original data before preprocessing
    print("\nBefore Preprocessing:")
    print("Sample Data:")
    print(df.head())
    print("\nMissing Values:", df.isnull().sum())
    print("Data Types:", df.dtypes)

    # Handle missing values and clean text
    df['statement'] = df['statement'].fillna('')  # Ensure no missing statements
    df['status'] = df['status'].fillna('Unknown')  # Ensure no missing statuses
    df['statement'] = df['statement'].astype(str).str.lower()
    df['statement'] = df['statement'].str.replace('[^\w\s]', '', regex=True)  # Updated to use regex=True

    # Print after cleaning text
    print("\nAfter text preprocessing:")
    print(df.head())

    # Define the correct order of classes
    class_order = ['Anxiety', 'Bipolar', 'Depression', 'Normal',
                  'Personality disorder', 'Stress', 'Suicidal']

    # Custom label encoding to maintain desired order
    le = LabelEncoder()
    le.fit(class_order)
    df['status_encoded'] = le.transform(df['status'])

    # Print the encoded labels
    print("\nTarget classes after encoding:", le.classes_)
    print("\nEncoded data (first 5 rows):")
    print(df[['status', 'status_encoded']].head())

    # Vectorize text
    vectorizer = TfidfVectorizer(max_features=100)
    X = vectorizer.fit_transform(df['statement']).toarray()
    y = df['status_encoded']

    # Print the shape of X and y
    print("\nShape of X (features) and y (target):", X.shape, y.shape)

    return X, y  # Only return X and y

# Example usage
# Assuming df is your DataFrame loaded previously:
# X, y = preprocess_data(df)  # Process data and get X, y

df = load_data()  # Assuming load_data is a function that loads your dataset
X, y = preprocess_data(df)  # Process and print debug output

# 4.1 SVM
def run_svm(X, y):
    from sklearn.model_selection import train_test_split
    from sklearn.svm import SVC
    from sklearn.metrics import accuracy_score, classification_report

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

    svm = SVC()
    svm.fit(X_train, y_train)
    y_pred = svm.predict(X_test)

    print("\nSVM Results:")
    print("Accuracy:", accuracy_score(y_test, y_pred))
    print("\nDetailed Report:")
    print(classification_report(y_test, y_pred))

run_svm(X, y)

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

def run_random_forest(X, y):
    # Split data into training and testing sets (70:30 split)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

    # Initialize Random Forest model
    rf = RandomForestClassifier()

    # Train the model
    rf.fit(X_train, y_train)

    # Predict on test data
    y_pred = rf.predict(X_test)

    # Print results
    print("\nRandom Forest Results:")
    print("Accuracy:", accuracy_score(y_test, y_pred))
    print("\nDetailed Report:")
    print(classification_report(y_test, y_pred))

run_random_forest(X, y)

def run_naive_bayes(X, y):
    from sklearn.model_selection import train_test_split
    from sklearn.naive_bayes import GaussianNB
    from sklearn.metrics import accuracy_score, classification_report

    # Split data into training and testing sets (70:30 split)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

    # Initialize Naive Bayes model
    nb = GaussianNB()

    # Train the model
    nb.fit(X_train, y_train)

    # Predict on test data
    y_pred = nb.predict(X_test)

    # Print results
    print("\nNaive Bayes Results:")
    print("Accuracy:", accuracy_score(y_test, y_pred))
    print("\nDetailed Report:")
    print(classification_report(y_test, y_pred))

run_naive_bayes(X, y)

# 4.4 KNN
def run_knn(X, y):
    from sklearn.model_selection import train_test_split
    from sklearn.neighbors import KNeighborsClassifier

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

    knn = KNeighborsClassifier()
    knn.fit(X_train, y_train)
    y_pred = knn.predict(X_test)

    print("\nKNN Results:")
    print("Accuracy:", accuracy_score(y_test, y_pred))
    print("\nDetailed Report:")
    print(classification_report(y_test, y_pred))

run_knn(X, y)

def plot_individual_accuracy(X, y):
    import matplotlib.pyplot as plt
    from sklearn.model_selection import train_test_split
    from sklearn.svm import SVC
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.naive_bayes import GaussianNB
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.metrics import accuracy_score

    models = {
        'SVM': SVC(),
        'Random Forest': RandomForestClassifier(),
        'Naive Bayes': GaussianNB(),
        'KNN': KNeighborsClassifier()
    }

    accuracies = []
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

    for name, model in models.items():
        model.fit(X_train, y_train)
        acc = accuracy_score(y_test, model.predict(X_test))
        accuracies.append(acc)

    # Plot the results
    plt.figure(figsize=(10, 6))
    plt.bar(models.keys(), accuracies)
    plt.title('Model Accuracy Comparison')
    plt.ylabel('Accuracy')
    plt.xticks(rotation=45)
    plt.show()

plot_individual_accuracy(X, y)

def evaluate_split_ratios(X, y):
    from sklearn.model_selection import train_test_split
    from sklearn.svm import SVC
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.naive_bayes import GaussianNB
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.metrics import accuracy_score
    import matplotlib.pyplot as plt

    # Test sizes for 60:40, 70:30, 80:20 splits
    test_sizes = [0.4, 0.3, 0.2]
    models = {
        'SVM': SVC(),
        'Random Forest': RandomForestClassifier(),
        'Naive Bayes': GaussianNB(),
        'KNN': KNeighborsClassifier()
    }

    results = {}

    for test_size in test_sizes:
        train_size = 1 - test_size
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)

        print(f"\nResults for {int(train_size*100)}:{int(test_size*100)} split")
        print("-" * 50)

        results[f"{int(train_size*100)}:{int(test_size*100)}"] = {}

        for name, model in models.items():
            model.fit(X_train, y_train)
            y_pred = model.predict(X_test)
            accuracy = accuracy_score(y_test, y_pred)
            results[f"{int(train_size*100)}:{int(test_size*100)}"][name] = accuracy
            print(f"{name} Accuracy: {accuracy:.4f}")

    # Visualization
    plt.figure(figsize=(12, 6))
    x = range(len(models))
    width = 0.25

    for i, (split, scores) in enumerate(results.items()):
        plt.bar([j + i*width for j in x],
               list(scores.values()),
               width,
               label=f'Split {split}')

    plt.xlabel('Models')
    plt.ylabel('Accuracy')
    plt.title('Model Performance Across Different Train-Test Splits')
    plt.xticks([i + width for i in x], list(models.keys()), rotation=45)
    plt.legend()
    plt.tight_layout()
    plt.show()

    return results

results = evaluate_split_ratios(X, y)